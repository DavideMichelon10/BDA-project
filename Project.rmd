---
title: "BDA - Project"
author: "Anonymous"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 1
urlcolor: blue
---

```{r setup, include=FALSE}
# This chunk sets echo = TRUE as default, that is print all code.
# knitr::opts_chunk$set can be used to set other notebook generation options, too.
# include=FALSE inside curly brackets makes this block not be included in the pdf.
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}

library(pracma)
library(posterior)
library(cmdstanr)
library(rstan)
library(ggplot2)
library(loo)
library(dplyr)
library(tidyr)
library(brms)
library(firatheme)
# library(tidyverse)

cmdstanr::check_cmdstan_toolchain(fix = TRUE)

register_knitr_engine(override = FALSE)
```

# Introduction
Nowadays, given the enormous amount of data that is generated,  it is essential to be able to perform precise analyzes in order to have an important advantage over competitors.In particular, data analysis is proving increasingly important in the world of sport and currently most of high-level coaches rely on data scientists. In this report, we will focus on the MotoGP which is the premier class of motorcycle road racing events held on road circuits. 


## Motivation

predirre vincitore moto gp basandosi sulle sue skill e sulla sua moto

## Problem

(a) what is the relative influence of the driver and constructor on race results,
(b) how do drivers rank in terms of skill level, and (c) how do constructors rank in terms of
race car advantage?

# Data

## Data Preparation

```
test_data_driver <- data.frame(
  rider_name = data$rider_name,
  posx = data$prop_trans,
  sequence = data$year*data$sequence
  
)

driver_prop <- test_data_driver %>% pivot_wider(names_from = rider_name, values_from = posx)

test_data_constr <- data.frame(
  team_name = data$team_name,
  pos = data$prop_trans,
  sequence = data$year*data$sequence
  
)

team_prop <- test_data_constr %>% pivot_wider(names_from = team_name, values_from = pos) 



for(i in c(2:ncol(team_prop))){
  temp <- team_prop[,i]
  team_prop[,i] = lapply(temp, function(x) lapply(x, mean))
}

save(team_prop,file="team_prop.Rda")
save(driver_prop,file="driver_prop.Rda")
```

```{r}
load("./team_prop.Rda")
load("./driver_prop.Rda")
```


## Some EDA ----

```{r}
data = read.csv("./data/race_results_view.csv")
```

```{r}
# Data processing 
## Restricting my analysis to the period 2012-2021
data <- data %>% filter(
  position > 0,
  year > 2011
)
## convert to factors
data <- data %>% mutate(
  rider_name  = as.factor(rider_name),
  team_name  = as.factor(team_name)
)

# New variables
data <- data %>% group_by(year, sequence) %>% mutate(  
  position_prop = (n() - position) / (n() - 1),        
  prop_trans = (position_prop * (n() - 1) + 0.5) / n() 
  )
```
The variable position_prop represents how many rider_names you beat in a race, the variables position_trans is a transformation (see https://stats.stackexchange.com/a/134297/116878)

Preliminary plots


```{r}
## finish position
ggplot(data, aes(x = factor(position))) +
  geom_bar(fill = "darkmagenta") +
  labs(
    title = "Distribution of finish positions",
    subtitle = "Era (2012-2021)",
    x = "Finish position",
    y = "Count"
  )
```
Dire che non ce ne sono sempre 22 perchÃ¨ non tutti arrivano in fondo

```{r}
data %>%
  filter(rider_name %in% c("Rossi, Valentino", "Quartararo, Fabio", "Marquez, Marc", "Lorenzo, Jorge")) %>%
  ggplot(aes(x = factor(position), fill = rider_name)) +
  geom_bar(position = position_dodge(preserve = "single")) +
  scale_x_discrete(limits=rev,breaks=seq(1, 23, 3)) +
  labs(
    x = "Finish position",
    y = "Count",
    title = "Different rider's finish positions",
    subtitle = "Conditional on finishing the race",
    fill = ""
  ) +
  theme(legend.position = "top") +
  facet_wrap(~year)
```
Dire che non tutti i piloti sono sempre presenti (alcuni si ritirano, altri arrivano dopo)

```{r}
data %>%
  filter(rider_name %in% c("Rossi, Valentino", "Crutchlow, Cal", "Marquez, Marc")) %>%
  ggplot(aes(x = prop_trans, fill = rider_name)) +
  geom_density(alpha = 0.5, bw = 0.1) +
  labs(
    x = "Smoothed proportion of outperformed rider_names",
    y = "Density",
    title = "Different rider_names' results",
    subtitle = "Proportion of finished drivers outperformed",
    fill = ""
  ) +
  theme(legend.position = "top", axis.text.x = element_text(angle = 45, vjust = 0.85)) +
  facet_wrap(~year)
```

# Description of the models

spiegare cosa rappresentano i vari beta

Separate model:
$\begin{align*} y_{dc} \sim {\sf Beta}(\mu_{dc},\Phi) \\ logit(\mu_{dc}) = \beta_{d}  + \beta_{c}  \\ \beta_{d} \sim {\mathcal N}(0,10) \\  \beta_{c} \sim {\mathcal N}(0,10) \end{align*}$

Hierarchical model:
$\begin{align*} y_{dcs} \sim {\mathcal Beta}(\mu_{dcs},\Phi) \\ logit(\mu_{dcs}) = \beta_{d} + \beta_{ds} + \beta_{c} + \beta_{cs} \\ \beta_{d} \sim {\mathcal N}(0,\sigma^{2}_{d}) \\ \beta_{ds} \sim {\mathcal N}(0,\sigma^{2}_{ds}) \\ \beta_{c} \sim {\mathcal N}(0,\sigma^{2}_{c}) \\ \beta_{cs} \sim {\mathcal N}(0,\sigma^{2}_{cs}) \\ \sigma_{d} \sim {\mathcal \Gamma}(1,1) \\ \sigma_{ds} \sim {\mathcal \Gamma}(1,1) \\ \sigma_{c} \sim {\mathcal \Gamma}(1,1) \\ \sigma_{cs} \sim {\mathcal \Gamma}(1,1) \end{align*}$

## Priors

weakly ...

# Stan models

{output.var="hierarchical_model_d"}
data { 
  int<lower=0> N; #number of races
  int<lower=0> J_riders; #number of riders
  matrix[N, J_riders] y_riders; 
}

parameters {
  vector[J_riders] mu_riders; 
  real <lower=0> sigma;
  real <lower=0> sigma_hyp;
}

transformed parameters {
  vector[J_riders] mu = inv_logit(mu_riders);
}

model {
  //Priors
  for (j in 1:J_riders){
      mu_riders[j] ~ normal(0, sigma_hyp);
  }
  
  sigma ~ gamma(1,1);
  
  //Likelihood
  for (j in 1:J_riders){
    //mu[i] = inv_logit(mu_teams[i])
    y_riders[,j] ~ beta(mu[j]*sigma, (1-mu[j])*sigma);
  }

}

```
list_drivers = list(y_riders = driver_prop, N = nrow(driver_prop), J_riders = ncol(driver_prop))
# fit_driver <- hierarchical_model_d$sample(data = list_drivers, refresh=1000)

# basic model
fit_basic <- brm(
  formula = prop_trans ~ 0 + (1 | rider_name) + (1 | team_name),
  family  = Beta(),
  data    = data,
  backend = "cmdstanr",
  chains  = 4,
  cores   = 6,
  threads = 3,
  warmup  = 1000,
  iter    = 3500
)

write_rds(fit_basic, "./fit/fit_basic.rds")
```

```{r}
fit_basic = readRDS("./fit/fit_basic.rds") %>% add_criterion("loo")
summary(fit_basic)
```


```
# year model
fit_year <- brm(
  formula = prop_trans ~ 0 + (1 | rider_name) + (1 | team_name) + (1 | rider_name:year) + (1|team_name:year),
  family  = Beta(),
  data    = data,
  backend = "cmdstanr",
  chains  = 4,
  cores   = 6,
  threads = 2,
  warmup  = 1000,
  iter    = 3500
)
```


```{r}
fit_year = readRDS("./fit/fit_year.rds") %>% add_criterion("loo")
summary(fit_year)
```

```{r}
library(nlmeU)
library(corrplot)
library(nlme)
library(lattice)
library(plot.matrix)
library(lme4)
library(insight)
a = ranef(fit_year)
dotplot(a$rider_name)
```


```{r}
mcmc_plot(fit_year, type = "trace") +
  facet_wrap(~parameter, nrow = 6, scales = "free")
```


# Rhat convergence diagnostics and interpretation

Basic model

```{r}
rhats <- rhat(fit_basic)
any(rhats[!is.nan(rhats)] > 1.01)
```

Year model

```{r}
rhats <- rhat(fit_year)
any(rhats[!is.nan(rhats)] > 1.01)
```

#  HMC specific convergence diagnostics

# Effective sample size diagnostic (n_eff)

# Posterior predictive checking and interpretation 

```{r}
# 2019 posterior predictive check ----
# create drivers & constructors in 2019
pred_tab <-
  data %>%
  filter(year == 2019) %>%
  select(rider_name, team_name, year)

# predict proportion of outperformed drivers
pp_tab <- posterior_predict(fit_year, pred_tab)

## Proportion plot ----
# yrep
pred_tab_long <-
  pred_tab %>%
  bind_cols(t(pp_tab) %>% as_tibble(.name_repair = "minimal") %>% 
  set_names(1:10000)) %>%
  pivot_longer(
    cols      = c(-rider_name, -team_name, -year),
    names_to  = "sample",
    values_to = "prop_trans"
  ) %>%
  mutate(origin = "simulated")

# y
true_tab_long <-
  data %>%
  filter(year == 2019) %>%
  select(rider_name, team_name, year, prop_trans) %>%
  mutate(origin = "observed")

ordered_levels <-
  true_tab_long %>%
  group_by(rider_name) %>%
  summarise(prop = mean(prop_trans)) %>%
  arrange(-prop) %>%
  pull(rider_name) %>%
  as.character()


bind_rows(pred_tab_long, true_tab_long) %>%
  ggplot(aes(x = prop_trans, fill = origin)) +
  geom_density(alpha = 0.8, bw = .07) +
  facet_wrap(~factor(rider_name, levels = ordered_levels), scales = "free") +
  xlim(0, 1) +
  theme_fira() +
  scale_fill_fira() +
  theme(legend.position = "top") +
  labs(
    title = "2019 season posterior predictive check",
    x = "Proportion of outperformed drivers",
    y = "",
    fill = ""
  )

```


# Model comparison and interpretation of the results

```{r}
loo_results <- loo_compare(
  fit_basic,
  fit_year,
  model_names = c("Driver + constructor", "Driver + constructor + year")
)

loo_results
```


# Predictive performance assessment

# Alternative priors testing

# Problems and potential improvements

# Conclusion

# Self-reflection about what the group learned 