---
title: "BDA - Project"
author: "Anonymous"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 1
urlcolor: blue
---

```{r setup, include=FALSE}
# This chunk sets echo = TRUE as default, that is print all code.
# knitr::opts_chunk$set can be used to set other notebook generation options, too.
# include=FALSE inside curly brackets makes this block not be included in the pdf.
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}

library(posterior)
library(loo)
library(brms)
library(nlmeU)
library(corrplot)
library(nlme)
library(lattice)
library(plot.matrix)
library(insight)
library(firatheme)
library(tidyverse)

cmdstanr::check_cmdstan_toolchain(fix = TRUE)

register_knitr_engine(override = FALSE)
```


# Introduction
Nowadays, given the enormous amount of data that is generated,  it is essential to be able to perform precise analyzes in order to have an important advantage over competitors. In particular, data analysis is proving increasingly important in the world of sport and currently most of high-level teams rely on data scientists. In this report, we will focus on the MotoGP sport which is the premier class of motorcycle road racing events held on road circuits. Using a public data-set of the MotoGP results from 2012 to 2021, we designed two different models to describe how  drivers and constructions can affect the result of the final world champion. We were curious to understand what is more relevant between drivers and construction teams and whether the chances of final victory for ordinary drivers with first-class teams are higher than normal teams with champion drivers. The first model takes into account only the drivers and construction teams but the results were not satisfactory. Indeed, we found out that the performance of the motorbikes differ according to the technological development and the same motorbike can have opposite performances from year to year. That said, in the second model we introduced the year variable and thus the results have improved significantly.


# Data
The dataset used is public on the website at this [link](https://observablehq.com/@piratus/motogp-results-database). It is an SQLite database with MotoGP race results and it contains 15 tables: bikes, categories, circuits, countries, gp_country_codes, race_conditions, race_results, races, riders, season_categories, season_results, seasons, stage_names, stages, and teams. By using a JavaScript script, we downloaded the table race_results_view where each row represents the finish position of a driver for a specific race. The fields that interest us are:
(a) year: race year.
(b) sequence: race number for a specific year. Every first race of every year has sequence equal to 1, the second race has sequence equal to 2 and so on.
(c) rider_name: name of the drive.
(d) team_name: name of the construction team.
(e) position: final position for a specific driver in a specific race.

## Data Preparation

## Some EDA ----

```{r}
data = read.csv("./data/race_results_view.csv")
```

```{r}
# Data processing 
## Restricting my analysis to the period 2012-2021
data <- data %>% filter(
  position > 0,
  year > 2011
)
## convert to factors
data <- data %>% mutate(
  rider_name  = as.factor(rider_name),
  team_name  = as.factor(team_name)
)

# New variables
data <- data %>% group_by(year, sequence) %>% mutate(  
  position_prop = (n() - position) / (n() - 1),        
  prop_trans = (position_prop * (n() - 1) + 0.5) / n() 
  )
```
The variable position_prop represents how many rider_names you beat in a race, the variables position_trans is a transformation: Indeed The documentation for the R betareg package mentions that: "if y also assumes the extremes 0 and 1, a useful transformation in practice is (y * (n−1) + 0.5) / n where n is the sample size".

(see https://stats.stackexchange.com/a/134297/116878)

Preliminary plots


```{r}
## finish position
ggplot(data, aes(x = factor(position))) +
  geom_bar(fill = "darkmagenta") +
  labs(
    title = "Distribution of finish positions",
    subtitle = "Era (2012-2021)",
    x = "Finish position",
    y = "Count"
  )
```


```{r}
data %>%
  filter(rider_name %in% c("Rossi, Valentino", "Quartararo, Fabio", "Marquez, Marc", "Lorenzo, Jorge")) %>%
  ggplot(aes(x = factor(position), fill = rider_name)) +
  geom_bar(position = position_dodge(preserve = "single")) +
  scale_x_discrete(limits=rev,breaks=seq(1, 23, 3)) +
  labs(
    x = "Finish position",
    y = "Count",
    title = "Different rider's finish positions",
    subtitle = "Conditional on finishing the race",
    fill = ""
  ) +
  theme(legend.position = "top") +
  facet_wrap(~year)
```
Dire che non tutti i piloti sono sempre presenti (alcuni si ritirano, altri arrivano dopo)

```{r}
data %>%
  filter(rider_name %in% c("Rossi, Valentino", "Crutchlow, Cal", "Marquez, Marc")) %>%
  ggplot(aes(x = prop_trans, fill = rider_name)) +
  geom_density(alpha = 0.5, bw = 0.1) +
  labs(
    x = "Smoothed proportion of outperformed rider_names",
    y = "Density",
    title = "Different rider_names' results",
    subtitle = "Proportion of finished drivers outperformed",
    fill = ""
  ) +
  theme(legend.position = "top", axis.text.x = element_text(angle = 45, vjust = 0.85)) +
  facet_wrap(~year)
```

# Description of the models

spiegare cosa rappresentano i vari beta

For driver d and constructor c , we specify the following generative multilevel model for the proportion of drivers beaten $y_{dc}$:

$\begin{align*} y_{dc} \sim {\sf Beta}(\mu_{dc},\Phi) \\ logit(\mu_{dc}) = \beta_{d}  + \beta_{c}  \\ \beta_{d} \sim {\mathcal N}(0,\sigma^{2}_{d}) \\  \beta_{c} \sim {\mathcal N}(0,\sigma^{2}_{c}) \\ \sigma_{d} \sim {\mathcal \Gamma}(1,1) \\ \sigma_{c} \sim {\mathcal \Gamma}(1,1) \\ \Phi \sim \Gamma(1,1) \end{align*}$

The Beta distribution of the model does not follow the standard $(\alpha,\beta)$ parametrization, but rather a Beta regression formulation with a mean parameter $\mu$ and a dispersion parameter $\Phi$ (Ferrari and Cribari-Neto, 2004).

In particular, for regression analysis it is useful to model the mean of the response. Also it is typical to define the model so that it contains a precision (dispersion) parameter. In order to do that we can define $$\mu = \frac{\alpha}{\alpha+\beta}$$ and $$\Phi = \alpha + \beta$$ so that $$E(y) = \mu$$ and $$var(y) = \frac{\mu(1-\mu)}{1+\Phi}$$. such that $\mu$ is the mean of the response variable and $\Phi$ can be interpreted as a precision parameter

The (hypothetical) average driver at an average team will on
average have $\mu_{dc}$ = 0, which translates into a probability of 0.5 of beating other drivers.

Then, $\Beta_{d}$ represents the mean driver skill as a log-odds ratio; e.g., if βd = 0:3, this means that the probability of beating other drivers is 1=(1 + e−0:3) ≈ 0:57.

Taking into account that the performances of a driver can change over years (e.g. for his age or for his experience), and that also the quality of the motorbike can depends on the season (e.g. for new technologies) $y_{dcs}$:

$\begin{align*} y_{dcs} \sim {\mathcal Beta}(\mu_{dcs},\Phi) \\ logit(\mu_{dcs}) = \beta_{d} + \beta_{ds} + \beta_{c} + \beta_{cs} \\ \beta_{d} \sim {\mathcal N}(0,\sigma^{2}_{d}) \\ \beta_{ds} \sim {\mathcal N}(0,\sigma^{2}_{ds}) \\ \beta_{c} \sim {\mathcal N}(0,\sigma^{2}_{c}) \\ \beta_{cs} \sim {\mathcal N}(0,\sigma^{2}_{cs}) \\ \sigma_{d} \sim {\mathcal \Gamma}(1,1) \\ \sigma_{ds} \sim {\mathcal \Gamma}(1,1) \\ \sigma_{c} \sim {\mathcal \Gamma}(1,1) \\ \sigma_{cs} \sim {\mathcal \Gamma}(1,1) \\ \Phi \sim \Gamma(1,1) \end{align*}$

So here we also included the seasonal driver form parameter βds, which represents yearly deviations from the long-term average driver skill and βcs, which represents yearly deviations from the long-term average constructor advantage.

## Priors

We choose a weakly informative for the $\Phi$ parameters and weakly informative hyperpriors for the $\sigma$ parameters in both the models.
Indeed a $\Gamma(1,1)$ is a common choice, since it is a distribution that starting from 0 start decreasing with a "Gaussian like trend": it behaves similar to the right tail of a Gaussian or a Cauchy centered in 0.  

```
list_drivers = list(y_riders = driver_prop, N = nrow(driver_prop), J_riders = ncol(driver_prop))
# fit_driver <- hierarchical_model_d$sample(data = list_drivers, refresh=1000)
```

```{r}
prior <- c(
    prior(gamma(1,1), class = sd),
    prior(gamma(1,1), class = phi)
   )
# basic model
fit_basic <- brm(
  formula = prop_trans ~ 0 + (1 | rider_name) + (1 | team_name),
  family  = Beta(),
  data    = data,
  prior = prior,
  backend = "cmdstanr",
  chains  = 4,
  cores   = 6,
  threads = 3,
  warmup  = 1000,
  iter    = 3500
)
write_rds(fit_basic, "./fit/fit_basic.rds")
```


```{r}
#fit_basic = readRDS("./fit/fit_basic.rds") %>% add_criterion("loo")
summary(fit_basic)
```


```{r}
# year model
fit_year <- brm(
  formula = prop_trans ~ 0 + (1 | rider_name) + (1 | team_name) + (1 | rider_name:year) + (1|team_name:year),
  family  = Beta(),
  prior = prior,
  data    = data,
  backend = "cmdstanr",
  chains  = 4,
  cores   = 6,
  threads = 2,
  warmup  = 1000,
  iter    = 3500
)
write_rds(fit_year, "./fit/fit_year.rds")
```

```{r}
#fit_year = readRDS("./fit/fit_year.rds") %>% add_criterion("loo")
summary(fit_year)
```

```{r}

a = ranef(fit_year)
dotplot(a$rider_name)
```


```{r}
mcmc_plot(fit_year, type = "trace") +
  facet_wrap(~parameter, nrow = 6, scales = "free")
```



# Rhat convergence diagnostics and interpretation

Basic model

```{r}
rhats <- rhat(fit_basic)
any(rhats[!is.nan(rhats)] > 1.01)
```

Year model

```{r}
rhats <- rhat(fit_year)
any(rhats[!is.nan(rhats)] > 1.01)
```

$\hat{R}$ is an estimate for potential scale reduction: it tell us if we are using the correct parameters. In particular as N->+$\infty$ it should decrease to 1.

Conceptually it represents the ratio between an overestimate of the marginal posterior variance (done using a linear combination between the within and the between variance) and the within variance.

If $\hat{R}$ is high, then we have reason to believe that proceeding with further simulations may improve our inference about the target distribution of the associated scalar estimand.

A rule of thumb is that if $\hat{R}$ < 1.01 we don't need to increase the number of simulations, so in this case everything is fine.

#  HMC specific convergence diagnostics

The HMC chains convergence can be determined by plotting the individual chains that have been executed for the main model parameters 

```{r}
hmc_conv <- mcmc_plot(fit_year, type = "trace") +
            facet_wrap(~parameter, nrow = 6, scales = "free") +
            scale_colour_brewer(type = "seq", palette = "Spectral")
            #theme_fira() +
            #scale_colour_fira()

hmc_conv
```
By visual inspection it can be seen how the chains for the different parameters have converged. 


# Effective sample size diagnostic (n_eff)

```{r}
summary(fit_basic)
summary(fit_year)
```
The ess_bulk function produces an estimated Bulk Effective Sample Size (bulk-ESS) using rank normalized draws whereas the ess_tail function produces an estimated Tail Effective Sample Size (tail-ESS) by computing the minimum of effective sample sizes for 5% and 95% quantiles.
For each parameter, both Bulk-ESS and Tail-ESS are bigger than 100 (approximately) per Markov Chain which means that they are reliable and indicate that estimates of respective posterior quantiles are reliable.



# Posterior predictive checking and interpretation 

Posterior predictive checks are an integral part of a Bayesian workflow, where we simulate the data we expect $\tilde{y}$ based on the model posterior, and we compare it with the observed data $y$, to see whether there's consistency. Basically, if $\tilde y$ is similar to $y$, then the model encapsulates the outcome well. In this case, we decided to run the posterior predictive checks on two different years, one in 2012 and one 2021 since there have been changes to both the rider roaster and teams between nine years. 

```{r}
# 2021 posterior predictive check ----

pred_tab <-
  data %>%
  filter(year == 2021) %>%
  filter(!(rider_name %in% c("Pedrosa, Dani","Gerloff, Garrett","Dixon, Jake"))) %>% 
  select(rider_name, team_name, year)

# predict proportion of outperformed drivers
pp_tab <- posterior_predict(fit_year, pred_tab)

## Proportion plot ----
# yrep
pred_tab_long <-
  pred_tab %>%
  bind_cols(t(pp_tab) %>% as_tibble(.name_repair = "minimal") %>% 
  set_names(1:10000)) %>%
  pivot_longer(
    cols      = c(-rider_name, -team_name, -year),
    names_to  = "sample",
    values_to = "prop_trans"
  ) %>%
  mutate(origin = "simulated")

# y
true_tab_long <-
  data %>%
  filter(year == 2021) %>%
  filter(!(rider_name %in% c("Pedrosa, Dani","Gerloff, Garrett","Dixon, Jake")))%>% 
  select(rider_name, team_name, year, prop_trans) %>%
  mutate(origin = "observed")

ordered_levels <-
  true_tab_long %>%
  group_by(rider_name) %>%
  summarise(prop = mean(prop_trans)) %>%
  arrange(-prop) %>%
  pull(rider_name) %>%
  as.character()


PPC_2021 <- bind_rows(pred_tab_long, true_tab_long) %>%
            ggplot(aes(x = prop_trans, fill = origin)) +
            geom_density(alpha = 0.8, bw = .07) +
            facet_wrap(~factor(rider_name, levels = ordered_levels), scales = "free") +
            xlim(0, 1) +
            #theme_fira() +
            #scale_fill_fira() +
            #theme(legend.position = "top") +
            labs(
              title = "Posterior predictive check",
              subtitle = "2021 season",
              x = "Proportion of outperformed riders",
              y = "",
              fill = ""
            )

PPC_2021
```
Note: In the 2021 season riders Dani Pedrosa, Jake Dixon and Gerloff Garrett participated in one race each, so they have been excluded from the posterior predictive check. 

```{r}
# 2012 posterior predictive check ----

pred_tab <-
  data %>%
  filter(year == 2012) %>%
  filter(!(rider_name %in% c("Rapp, Steve","Battaini, Franco","Aoyama, Hiroshi","Vermeulen, Chris",
                         "Salom, David","Rolfo, Roberto", "Yates, Aaron"))) %>% 
  select(rider_name, team_name, year)

# predict proportion of outperformed drivers
pp_tab <- posterior_predict(fit_year, pred_tab)

## Proportion plot ----
# yrep
pred_tab_long <-
  pred_tab %>%
  bind_cols(t(pp_tab) %>% as_tibble(.name_repair = "minimal") %>% 
  set_names(1:10000)) %>%
  pivot_longer(
    cols      = c(-rider_name, -team_name, -year),
    names_to  = "sample",
    values_to = "prop_trans"
  ) %>%
  mutate(origin = "simulated")

# y
true_tab_long <-
  data %>%
  filter(year == 2012) %>%
  filter(!(rider_name %in%c("Rapp, Steve","Battaini, Franco","Aoyama, Hiroshi","Vermeulen, Chris",
                         "Salom, David","Rolfo, Roberto", "Yates, Aaron"))) %>% 
  select(rider_name, team_name, year, prop_trans) %>%
  mutate(origin = "observed")

ordered_levels <-
  true_tab_long %>%
  group_by(rider_name) %>%
  summarise(prop = mean(prop_trans)) %>%
  arrange(-prop) %>%
  pull(rider_name) %>%
  as.character()


PPC_2012 <- bind_rows(pred_tab_long, true_tab_long) %>%
            ggplot(aes(x = prop_trans, fill = origin)) +
            geom_density(alpha = 0.8, bw = .07) +
            facet_wrap(~factor(rider_name, levels = ordered_levels), scales = "free") +
            xlim(0, 1) +
            #theme_fira() +
            #scale_fill_fira() +
            #theme(legend.position = "top") +
            labs(
              title = "Posterior predictive check",
              subtitle = "2012 season",
              x = "Proportion of outperformed riders",
              y = "",
              fill = ""
            ) +
            scale_colour_brewer(type = "seq", palette = "Spectral")

PPC_2012
```
Note: in the 2012 season riders Aaron Yates, Chris Vermeulen, David Salom, Franco Battaini, Hiroshi Aoyama, Steve Rapp & Roberto Rolfo have been excluded since each of them had taken part in one Grand Prix. 

Both the plots show acceptable simulated values for each individual rider. One important thing to note is that for consistent high-performers (ex. Quartararo Fabio - 2021, Lorenzo Jorge - 2012) and low-performers (Salvadori Lorenzo - 2021), the posterior predictive distribution is under-dispersed, indicating a high value for \phi in these cases. 
On the other hand, for riders in the midfield (Marquez Alex - 2021, Jonathan Rea - 2012), the posterior predictive distribution seems to be a bit over-dispersed (low value of $\phi$), since there is more variation in observed data. 
But it can be seen that in both cases, the mean estimates do not seem too biased (especially in the midfield) so, we conclude that the model fits the observed data satisfyingly. 

# Model comparison and interpretation of the results

```{r}
loo_results <- loo_compare(
  fit_basic,
  fit_year,
  model_names = c("Driver + constructor", "Driver + constructor + year")
)

loo_results
```

Comparing the models, the best one is the model described by drivers, constructors and years. Models are sorted in decreasing order: the first is the best whereas the last is the worst. The difference in ELPD (elpd_diff) between the two models is considerably high, this means that the 2 models have different predictive performance. Moreover, 
it is notable how taking the year into consideration leads to significantly better results.

# Predictive performance assessment

# Alternative priors testing

We tried to change the parameters of the $\Gamma$ distribution, using a distribution with a shorter support, so more informative

```{r}
prior2 <- c(
    prior(gamma(0.01,0.01), class = sd),
    prior(gamma(0.01,0.01), class = phi)
   )

# basic model
fit_basic2 <- brm(
  formula = prop_trans ~ 0 + (1 | rider_name) + (1 | team_name),
  family  = Beta(),
  data    = data,
  prior = prior2,
  backend = "cmdstanr",
  chains  = 4,
  cores   = 6,
  threads = 3,
  warmup  = 1000,
  iter    = 3500
)

summary(fit_basic2)
```

```{r}
fit_year2 <- brm(
  formula = prop_trans ~ 0 + (1 | rider_name) + (1 | rider_name:year) + (1 | team_name)  + (1|team_name:year),
  family  = Beta(),
  data    = data,
  prior = prior2,
  backend = "cmdstanr",
  chains  = 4,
  cores   = 6,
  warmup  = 1000,
  iter    = 3500
)

summary(fit_year2)
```

We can see that also with a smaller support, the results are almost the same

We then tried to use the standard priors in brms, that are showed in the output:

```{r}
# basic model
fit_basic3 <- brm(
  formula = prop_trans ~ 0 + (1 | rider_name) + (1 | team_name),
  family  = Beta(),
  data    = data,
  backend = "cmdstanr",
  chains  = 4,
  cores   = 6,
  threads = 3,
  warmup  = 1000,
  iter    = 3500
)

prior_summary(fit_basic3)
```

```{r}
summary(fit_basic3)
```

```{r}
fit_year3 <- brm(
  formula = prop_trans ~ 0 + (1 | rider_name) + (1 | rider_name:year) + (1 | team_name)  + (1|team_name:year),
  family  = Beta(),
  data    = data,
  backend = "cmdstanr",
  chains  = 4,
  cores   = 6,
  warmup  = 1000,
  iter    = 3500
)

prior_summary(fit_year3)
```

```{r}
summary(fit_year3)
```

Also in this case, the changes are negligible

# Problems and potential improvements
During the project, we encountered some problems. The first issue was finding an idea and a reliable data set. After that, the initial idea was to use Stan with the data in the form of a matrix of observations and drivers However, this approach would have created multiple null entries (NA values) which are hard to handle using Stan. Therefore, we decided to switch to the brms library, which handles null entries automatically. The main difficulty was figuring out how to choose priors, in particular, how to define hyper-parameters. Moreover, defining priors in brms has been challenging and it took some time.


Although the model shows satisfactory results, a possible improvement could be to add more variables to the model. For example, a more in-depth analysis could be performed on the circuits, understanding which ones have common characteristics (e.g. circuits with many turns vs circuits with long straights) so as to be able to understand which are the weaknesses of motorbikes or riders and therefore where they should focus to improve.

Condizioni meteo?

# Conclusion

# Self-reflection about what the group learned 

Working on this project has been challenging, but at the same time stimulating and has allowed us to learn different notions about Bayesian logic. First of all, we learned how to work as a team in order to be as efficient as possible, we initially made an high-level analysis and we divided the tasks. After that, we met in person one last time before delivering to review everything and resolve the last doubts. Moreover, we have learned how to approach a statistical problem from start to finish, we believe that the final project has taught us to combine what we did with the assignments. In particular,we learned that data collection and data cleaning are essential to have a reliable and bias-free result.